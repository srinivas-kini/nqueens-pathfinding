# Assignment 0 (a0)
#### Srinivas Kini | skini@iu.edu | 2000773244
# Problem 1 : route_pichus.py

## _1.0 Understanding the Problem Statement_
#### The problem can be abstracted as a _Search Problem_, wherein an agent has to navigate through a _search space_ to accomplish a _goal_.
#### A _Search Problem_ can be formally defined as:
### - A set of states `S = {s0,s1...,sN}`
### - An initial state `s0 ∈ S`
### - A goal state `G ∈ S`
### - A Successor Function `SUCC(s)` which maps a given state `s ∈ S` to a set of states `F`, where `F ⊆ S`
### - A Cost Function `C` that can be interpreted as the amount of _energy_ the agent needs to spend to traverse between states in `S`.
### - We can also define a set of invalid states `X`, where `X ⊆ S`. Any state `x ∈ X` represents an obstacle which the agent cannot explore. In other words, `x ∉ F` which implies that `F` and `X` are disjoint sets.

## _2.0 Comments on the Starter Code_
#### The search space is represented using a matrix, where each state can be defined by a pair _(r,c)_ of coordinates along the x and y axes, where `r` is the index of the row and `c` is the index of the column. 
#### In the matrices represented by `map1.txt` and `map2.txt`, the initial state of the agent `p` is _(5,0)_.
#### The starter code provides a basic _Depth First Search_ strategy to explore the search space.
#### This is done by making use of a _Stack_ to pop and push the states. In the code, the variable `fringe` represents our stack, which contains a subset of states from the search space that the algorithm is exploring.
#### The `pop` method of the `list` class removes an element from the top of the `fringe`. And the `append` method adds an element to the top of the `fringe`, which corresponds to a _Last In First Out_ strategy of a stack.
#### The method `moves` acts as the successor function ` SUCC(S)`, which generates a set of subsequent states that the algorithm can explore by pushing it to the `fringe`.
#### The method `valid_index` restricts the successor function `moves` from generating invalid states. In this case, the invalid states may contain an index which exceeds the bounds of the search space, or any state `x ∈ X`  which is an obstacle the agent cannot traverse.
#### We can interpret the Cost Function `C` to be distributed uniformly across the search space. In this case, the cost for travelling from one state to another is 1. 
#### The cost function is stored as a tuple component for each tuple in the `fringe`, represented by the variable `curr_dist`.
`(curr_move, curr_dist) = fringe.pop() `
#### The algorithm terminates when it reaches one of the two conditions:
####  - It has successfully found a path from `s0` to `G`, where `G` is represented by the string `@`.
####  - The algorithm has exhausted all possible states of `S` and not found `G`.

## _3.0 Modifications and Implementation_
#### The problem can also be solved using a _Breadth First Search (BFS)_ strategy. 
#### While the overall implementation may not differ too much iteratively, BFS has the advantage of being a _complete_ algorithm. That is, it will eventually terminate, and not enter an iterative or recursive loop.
#### _The following changes have been made to the initial starter code:_
#### - The `fringe` now acts as a _Queue_. That is, each state is added from the rear end of the `fringe` and removed from the front.
#### - This can be simulated by using a `list` and passing the index _0_ to the `pop` method.
`(curr_move, curr_dist, pos) = fringe.pop(0) `
#### - Each tuple stored in the `fringe` represents a state `s ∈ S`, and carries an additional component that represents the relative direction from the previous state, represented by `pos`. 
#### -This component is stored as a character which encodes the directions _Up, Down, Left and Right_ as shown below:
`    moves = ((row + 1, col, 'D'), (row - 1, col, 'U'), (row, col - 1, 'L'), (row, col + 1, 'R'))`
#### - We also maintain a `set` of _visited_ states called `visited`. These are the states that have been explored, and revisiting them leads to more computational cost. 
#### - Each state generated by the successor function `moves` is evaluated against `visited`. It is appended to the `fringe` only if it is not present in `visited`. Similarly, an element is added to `visited` is removed from the `fringe`.
#### - The termination conditions for the algorithm remain the same, the caveat being that having a set of visited states facilitates quicker termination and is a more efficient implementation.

## _4.0 Complexity Analysis_
#### There are a couple of approaches to analyzing the running time and memory requirements of a BFS strategy.
### _Time Complexity_
### Approach 1
#### Let us assume our graph has a depth `d`. The depth represents the longest path from the initial state to any other state in the graph, which may or may not be a goal state.
#### Let `b` be the branching factor of the graph. The branching factor represents the maximum number of states the successor function can generate. In the case of our problem statement, the branching factor is 4, since the maximum possible number of states from a given state is Up,Down,Left or Right.
### The running time is then a function in the order of: `O(b^d)`

### Approach 2
#### Let `N` be the set of nodes in the graph. In the worst case, the BFS strategy explores the entire search space, where each node `n ∈ N` is pushed and popped from the queue at least once.
#### At each iteration, the successor function evaluates `|E|` edges (where `E` is the set of edges) corresponding to a node `n ∈ N` to generate the set of successor nodes.
### This results in a running time that is a function in the order of: `O(|N| + |E|)`

### _Space Complexity_
### Approach 1
#### Continuing with the notations used earlier, if the depth of the graph is `d` and the branching factor is `b`: 
### The fringe won't exceed a size greater than an order of: `O(b^d)`

### Approach 2
#### The previous statement can be rephrased by saying that the fringe will hold at most `|N|` nodes.
### Resulting in a space complexity in the order of: `O(|N|)`

# Problem 2 : arrange_pichus.py
## _1.0 Understanding the Problem Statement_
#### Much like Problem 1, Problem 2 can also be abstracted as a _Search Problem_.
#### We again consider:
### - A set of states `S = {s0,s1...,sN}`
### - An initial state `s0 ∈ S`
### - A Successor Function `SUCC(s)` which maps a given state `s ∈ S` to a set of states `F`, where `F ⊆ S`
### - In this case, the cost function `C` cannot be assigned a numerical value, unless a heuristic is defined.
### We consider a set of agents `P = {p0,p1,...,pN}`, where `|P| = k`. Where each agent `p ∈ P` is assigned a coordinate on the state graph.
### Again, we can represent the search space as a set of instances of the graph (matrix), with `n <= k` agents occupying it at any given time. The agents have to be placed on coordinates of the matrix in a linear fashion (one at a time), such that no other agent can _see_ it.
### Our goal is to return the set of states `G`, such that no agent `pi ∈ P` can _see_ another agent `pj ∈ P`. `pI` is said to _see_ `pJ` if:
#### - It is placed in the same row as `pJ` with no obstacles in between them.
#### - It is placed in the same column as `pJ` with no obstacles in between them.
#### - It is placed in the same diagonal as `pJ` with no obstacles in between them.


## _2.0 Comments on the Starter Code_
#### Like the previous problem, the started code implements  _Depth First Search_ strategy. The `fringe` acts a `Stack` and uses a _Last In First Out_ strategy to visit the nodes in the search space.
#### The successor function `successors` takes the search space as the input, which is stored in the variable `house_map`. The successors are limited by the constraint `house_map[r][c] == '.'`, which limits the successor function from placing the agent on obstacles.

## _3.0 Modifications and Implementation_
#### The crux of the implementation depends on calculating the _influence_ (or visibility) of each agent, which is done by scanning the column, the row and the diagonals occupied by each agent using the method `get_pichus_influence`. This  returns a set of `invalid_states` which cannot be occupied by subsequent agents. This process is repeated for all agents currently occupying the matrix, as shown in the code snippet below.
```
pichu_locs = {(row_i, col_i) for col_i in range(len(house_map[0])) for row_i in range(len(house_map)) if
                  house_map[row_i][col_i] == "p"} # get pichus' location on the matrix

invalid_states = set()
for pichu_loc in pichu_locs:
    invalid_states = invalid_states.union(get_pichus_influence(pichu_loc, house_map)) # merge the set of invalid states for all pichus
```
#### We then modify the `return` statement of the `successors` method to filter out all the states in `invalid_states`.
```
return [add_pichu(house_map, r, c) for r in range(0, len(house_map)) for c in range(0, len(house_map[0])) if
            house_map[r][c] == '.' and (r, c) not in invalid_states] # pick the first available location greedily
```
#### The variable `seen_maps` stores the matrices as strings. For each lookup and entry in `seen_maps`, the matrix is converted into a string using the method `printable_house_map`.
```
if printable_house_map(new_house_map) not in seen_maps:
    fringe.append(new_house_map)
```
### Finally, the algorithm terminates when the fringe is exhausted and all states have been visited.
## _4.0 Comments on the runtime_
#### The complexity of a DFS strategy remains the same as that of BFS when we speak in terms of the branching factor `b` and the depth `d`.
### Which is: `O(b^d)`
#### The same is also true when speaking in terms of the set of nodes `N` and the set of edges `E`.
### Which is: `O(|N| + |E|)`
#### _Caveat_
#### On running the algorithm in practice, the value of `k` has a huge impact on the running time. With small changes in the value of `k`, the algorithm can take unexpectedly long to terminate. This results of this observation are tabulated below.
| Row size  | Colum size | k | Running Time (s) |
| ------------- | ------------- | -------- | ----------|
| 15  | 15  |    29      |     0.023s      |
| 15  | 15  |    30      |     0.023s      |
| 15  | 15  |    32      |     350s      |
